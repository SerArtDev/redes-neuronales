{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMHXxFZWYAh6ZYZ+yLTnVrW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SerArtDev/redes-neuronales/blob/main/redes_neuronales_introducci%C3%B3n.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introducción a redes neuronales artificiales\n",
        "\n",
        "Se quiere predecir la nota que obtendrá un estudiante basado en sus horas de estudio y de sueño.\n",
        "\n",
        "| Horas de sueño | Horas de estudio | Nota |\n",
        "|----------------|------------------|------|\n",
        "| 3              | 5                | 75   |\n",
        "| 5              | 1                | 82   |\n",
        "| 10             | 2                | 93   |\n"
      ],
      "metadata": {
        "id": "iZk7F7uh8rAp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X2Exssy_8iIq"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "x = np.array(([3, 5], [5, 1], [10, 2]), dtype=float)\n",
        "y = np.array(([75], [82], [93]), dtype=float)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Normalizamos los datos, de tal manera que estén en un rango entre 0 y 1. Además, con eso aseguramos tener datos independientes de unidades entre las variables predictoras y la de respuesta.\n",
        "\n",
        "$x_{norm} = \\frac{x}{x_{max}}$\n",
        "\n",
        "$y_{norm} = \\frac{y}{y_{max}}$\n",
        "\n",
        "Para este caso, sabemos que la nota máxima es 100, entonces ese puede ser $y_{max}$."
      ],
      "metadata": {
        "id": "Gt0OPCTQ-NSJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_norm = x / np.amax(x, axis=0)\n",
        "y_norm = y / 100"
      ],
      "metadata": {
        "id": "dSdYfufT9ceR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tenemos una capa de dos neuoras de entrada y una capa con una neurona de salida. Entre estas dos capas se encuentran las llamadas capas ocultas, que si son muchas con muchas neuronas se le llamaría deep learning. Para este caso, utilizaremos tres neuronas en la capa oculta.\n",
        "\n",
        "Entre la primera y segunda capa se tienen 6 conexiones (2x3) y entre la segunda y tercera capa hay 3 conexiones (3x1). Cada una de estas conexiones tiene un peso $w$. En la segunda capa, para cada neurona se calcula la activación $z$ como la suma entre los valores $x$ de cada neurona de la capa anterior por el peso de la sinapsis o conexión entre estas dos.\n",
        "\n",
        "Los pesos en la sinapsis se pueden representar de la forma:\n",
        "\n",
        "$$\n",
        "w^I =\n",
        "\\begin{bmatrix}\n",
        "w_{1,1} & w_{1,2} & w_{1,3} \\\\\n",
        "w_{2,1} & w_{2,2} & w_{2,3} \\\\\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "El primer subíndice de cada elemento es la neurona de la primer cada y el segundo es a la neura que esta está conectada, entonces, $w_{1,1}$ representa al peso de la conexión entre la primer neurona de la primer cada con la primera neurona de la segunda capa.\n",
        "\n",
        "Se puede representar $z$ de forma matricial.\n",
        "\n",
        " $ z^{II} = x*w^I$.\n",
        "\n",
        "\n",
        "A partir de $z$ se puede calcular la activación usando una función de activación $f$.\n",
        "\n",
        "$a^{II} = f(z^{II})$\n",
        "\n",
        "Para la siguiente capa, ya no se usa $x$, sino $a^{II}$.\n",
        "\n",
        "$z^{III}=a^{II}*w^{II}$\n",
        "\n",
        "Estas conexiones tienen los pesos $w^{II}$.\n",
        "\n",
        "$$\n",
        "w^{II} =\n",
        "\\begin{bmatrix}\n",
        "w_{1,1} \\\\\n",
        "w_{2,1} \\\\\n",
        "w_{3,1} \\\\\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "A este $z^{III}$ se le aplica la función de activación, obteniendo así el valor de salida de la red neuronal.\n",
        "\n",
        "$\\hat{y} = f(z^{III})$"
      ],
      "metadata": {
        "id": "FbYG9yNhBxTI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Neural_network(object):\n",
        "  def __init__(self):\n",
        "    self.input_layer_size = 2\n",
        "    self.output_layer_size = 1\n",
        "    self.hidden_layer_size = 3\n",
        "    # Pesos de las sinapsis\n",
        "    self.wi = np.random.randn(self.input_layer_size, self.hidden_layer_size)\n",
        "    self.wii = np.random.randn(self.hidden_layer_size, self.output_layer_size)\n",
        "\n",
        "  ## Se calcula el valor de salida de la red.\n",
        "  def forward(self, x):\n",
        "    self.zii = np.dot(x, self.wi)\n",
        "    self.aii = self.sigmoid(self.zii)\n",
        "    self.ziii = np.dot(self.aii, self.wii)\n",
        "    y_hat = self.sigmoid(self.ziii)\n",
        "    return y_hat\n",
        "\n",
        "\n",
        "  ## Función de activación Sigmoid\n",
        "  def sigmoid(self, z):\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "\n",
        "  ## ----------------------Optimización------------------------------\n",
        "\n",
        "  def sigmoid_prime(self, z):\n",
        "    ## Derivdad de la función de activación Sigmoid\n",
        "    return np.exp(-z) / ((1 + np.exp(-z))**2)\n",
        "\n",
        "\n",
        "  def cost_funtion(self, x, y):\n",
        "    self.y_hat = self.forward(x)\n",
        "    return 0.5 * sum((y - self.y_hat)**2)\n",
        "\n",
        "\n",
        "  def cost_function_prime(self, x, y):\n",
        "    # Cálculo de derivada respecto a wi y wii\n",
        "    self.y_hat = self.forward(x)\n",
        "    deltaiii = np.multiply(-(y-self.y_hat), self.sigmoid_prime(self.ziii))\n",
        "    dJdwii = np.dot(self.aii.T, deltaiii)\n",
        "\n",
        "    deltaii = np.dot(deltaiii, self.wii.T) * self.sigmoid_prime(self.zii)\n",
        "    dJdwi = np.dot(x.T, deltaii)\n",
        "\n",
        "    return dJdwi, dJdwii\n",
        "\n",
        "  ## --------------Descenso de gradiente numérico------------------\n",
        "  def get_params(self):\n",
        "    params = np.concatenate((self.wi.ravel(), self.wii.ravel()))\n",
        "    return params\n",
        "\n",
        "  def set_params(self, params):\n",
        "    wi_start = 0\n",
        "    wi_end = self.hidden_layer_size * self.input_layer_size\n",
        "    self.wi = np.reshape(params[wi_start:wi_end],\n",
        "                        (self.input_layer_size, self.hidden_layer_size)\n",
        "                        )\n",
        "\n",
        "    wii_end = wi_end + self.hidden_layer_size*self.output_layer_size\n",
        "    self.wii = np.reshape(params[wi_end:wii_end],\n",
        "                        (self.hidden_layer_size, self.output_layer_size)\n",
        "                        )\n",
        "\n",
        "  def compute_gradients(self, x, y):\n",
        "    dJwi, dJdwii = self.cost_function_prime(x, y)\n",
        "    return np.concatenate(dJwi.ravel(), dJdwii.ravel())\n",
        "\n",
        "\n",
        "  def compute_numerical_gradient(self, x, y):\n",
        "    params_initial = self.get_params()\n",
        "    numgrad = np.zeros(params_initial.shape)\n",
        "    perturb = np.zeros(params_initial.shape)\n",
        "    e = 1e-4\n",
        "\n",
        "    for p in range(len(params_initial)):\n",
        "      # Vector de perturbación\n",
        "      perturb[p] = e\n",
        "      self.set_params(params_initial + perturb)\n",
        "      loss2 = self.cost_funtion(x, y)\n",
        "      self.set_params(params_initial - perturb)\n",
        "      loss1 = self.cost_funtion(x, y)\n",
        "      # Calcular el gradiente\n",
        "      numgrad[p] = (loss2 - loss1) / (2*e)\n",
        "      perturb[p] = 0\n",
        "\n",
        "    self.set_params = params_initial\n",
        "\n",
        "    return numgrad\n",
        "\n",
        "  def optimize(self, x, y, alpha):\n",
        "    while self.cost_funtion(x,y) > 10e-2:\n",
        "      print(self.cost_funtion(x,y))\n",
        "      self.set_params(self.get_params() + alpha * self.compute_numerical_gradient(x, y))\n"
      ],
      "metadata": {
        "id": "RRJfZky1A6KF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nn = Neural_network()\n",
        "y_hat = nn.forward(x)\n",
        "print(y_hat)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6zCkIz5Uaah-",
        "outputId": "e3114112-3f9e-4328-f4f8-077a977e192c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.28535098]\n",
            " [0.32615166]\n",
            " [0.31820229]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optimización\n",
        "Los valores obtenidos dependen de los pesos de las sinapsis y estos deben ser ajustados optimizando el modelo, de forma que pueda predecir valores de salida con mayor exactitud. Para esto, comparamos los valores de saluda reales ($y$) con los valores calculados con la red ($\\hat{y}$) para sus valores correspondientes de entrada ($x$).\n",
        "\n",
        "$ J = \\sum \\frac{1}{2}(y-\\hat{y})^2 $\n",
        "\n",
        "J es la función de costo y este debe ser minimizada ajustando $w^{I}, w^{II}$.\n",
        "\n",
        "$\\hat{y} = f(f(x*w^I)*w^{II})$\n",
        "\n",
        "$J = \\sum \\frac{1}{2}(y-f(f(x*w^I)*w^{II}))^2$\n",
        "\n",
        "El método de descenso de gradiente es el comúnmente usado para optimizar(entrenar) redes neuronales. Para este es necesario calcular las derivadas parciales de la función de costo respecto a $w^I, w^{II}$. Esto se logra aplicando la regla de la cadena múltiples veces y, en parte, por esto se llama backpropagation.\n",
        "\n",
        "$\\frac{\\delta J}{\\delta w^{II}}=(a^{II})^T\\delta^{III}$\n",
        "\n",
        "$\\delta^{III}=-(y-\\hat{y})f'(z^{III})$\n",
        "\n",
        "$\\frac{\\delta J}{\\delta w^I}=x^T\\delta^{II}$\n",
        "\n",
        "$\\delta^{II}=\\delta^{III}(W^{II})^Tf'(z^{II})$"
      ],
      "metadata": {
        "id": "Psl0YV0Kg-jv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nn = Neural_network()\n",
        "cost1 = nn.cost_funtion(x, y)\n",
        "#dJdwi, dJdwii = nn.cost_function_prime(x, y)\n",
        "\n",
        "print(cost1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WONeCbZGaqMc",
        "outputId": "2a56f64b-96e6-4fb0-d55c-4f99960cbca6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[10272.58582891]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A partir de estos gratientes se pueden obtener valores óptimos para las $w$.\n",
        "\n",
        "$w^I_1=w^I_0+\\alpha \\frac{\\delta J}{\\delta w^I_0}$\n",
        "\n",
        "$w^{II}_1=w^{II}_0+\\alpha \\frac{\\delta J}{\\delta w^{II}_0}$"
      ],
      "metadata": {
        "id": "cI9WzklcOaP8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nn.optimize(x, y, 1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 376
        },
        "id": "4HjyWJbpItui",
        "outputId": "6e4517fb-3dca-4ed3-e27a-b4bf21bad68f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[10272.58582891]\n",
            "[10499.]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-53c69c9c3bcf>:83: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  numgrad[p] = (loss2 - loss1) / (2*e)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "'numpy.ndarray' object is not callable",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-e968569e234e>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-3-53c69c9c3bcf>\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self, x, y, alpha)\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcost_funtion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m10e-2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcost_funtion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0malpha\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_numerical_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-3-53c69c9c3bcf>\u001b[0m in \u001b[0;36mcompute_numerical_gradient\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m     76\u001b[0m       \u001b[0;31m# Vector de perturbación\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m       \u001b[0mperturb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams_initial\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mperturb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m       \u001b[0mloss2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcost_funtion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams_initial\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mperturb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'numpy.ndarray' object is not callable"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EOWnR_Y8CKZk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}